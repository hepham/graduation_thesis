{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hepham/graduation_thesis/blob/main/ANN_using_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C5x_GxIiYvH7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from functools import reduce\n",
        "import requests\n",
        "import keras\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RhtFMrYoXJ5",
        "outputId": "23b08454-6939-427b-d746-5596d88589f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "path_train='/content/gdrive/MyDrive/datakltn/datatest.csv'\n",
        "path_test='/content/gdrive/MyDrive/datakltn/area0/test.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQWH6603oZvO",
        "outputId": "95e1f2de-e480-41e2-dc51-53c2e546672c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 48600 entries, 0 to 48599\n",
            "Data columns (total 21 columns):\n",
            " #   Column          Non-Null Count  Dtype\n",
            "---  ------          --------------  -----\n",
            " 0   Position Label  48600 non-null  int64\n",
            " 1   Beacon1         48600 non-null  int64\n",
            " 2   RSSI1           48600 non-null  int64\n",
            " 3   Beacon2         48600 non-null  int64\n",
            " 4   RSSI2           48600 non-null  int64\n",
            " 5   Beacon3         48600 non-null  int64\n",
            " 6   RSSI3           48600 non-null  int64\n",
            " 7   Beacon4         48600 non-null  int64\n",
            " 8   RSSI4           48600 non-null  int64\n",
            " 9   Beacon5         48600 non-null  int64\n",
            " 10  RSSI5           48600 non-null  int64\n",
            " 11  Beacon6         48600 non-null  int64\n",
            " 12  RSSI6           48600 non-null  int64\n",
            " 13  Beacon7         48600 non-null  int64\n",
            " 14  RSSI7           48600 non-null  int64\n",
            " 15  Beacon8         48600 non-null  int64\n",
            " 16  RSSI8           48600 non-null  int64\n",
            " 17  Beacon9         48600 non-null  int64\n",
            " 18  RSSI9           48600 non-null  int64\n",
            " 19  Beacon10        48600 non-null  int64\n",
            " 20  RSSI10          48600 non-null  int64\n",
            "dtypes: int64(21)\n",
            "memory usage: 7.8 MB\n"
          ]
        }
      ],
      "source": [
        "data_train=pd.read_csv(path_train)\n",
        "data_test=pd.read_csv(path_test)\n",
        "data_train.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "N7Gmuzkw84eq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2NF1yLyoclJ",
        "outputId": "dc5e9ead-2edb-4225-e5d5-6337fcd2ebc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (48600, 21)\n"
          ]
        }
      ],
      "source": [
        "data_train.head()\n",
        "print(\"shape:\",data_train.shape)\n",
        "data_train=data_train.abs()\n",
        "data_test=data_test.abs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9ltjsGk1X1kS"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.weights = np.zeros(shape=(input.shape[1],80))\n",
        "        bias = np.zeros(shape=(80,)) \n",
        "        pass\n",
        "    \n",
        "    def forward(self, input):\n",
        "        output = np.matmul(input, self.weights) + bias\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "syIY7QSEciiw"
      },
      "outputs": [],
      "source": [
        "class Dense(Layer):\n",
        "    def __init__(self, input_units, output_units, learning_rate=0.005):\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        # weight là ma trận có số cột = input số hàng là output\n",
        "        self.weights = np.random.randn(input_units, output_units)*0.01\n",
        "        self.biases = np.zeros(output_units)\n",
        "        \n",
        "    #hàm forward: f_l=w*f_(l-1)+bias\n",
        "    def forward(self,input):\n",
        "        return np.matmul(input, self.weights) + self.biases\n",
        "      \n",
        "    def backward(self,input,grad_output):\n",
        "      # đạo hàm df/dx,df/dw,df/d_bias\n",
        "        # compute d f / d x = d f / d dense * d dense / d x\n",
        "        # where d dense/ d x = weights transposed\n",
        "        df = np.dot(grad_output,np.transpose(self.weights))\n",
        "\n",
        "        # compute gradient w.r.t. weights and biases\n",
        "        dw = np.transpose(np.dot(np.transpose(grad_output),input))\n",
        "        dbias = np.sum(grad_output, axis = 0)\n",
        "        \n",
        "        # Xuống đồi bằng đạo hàm:\n",
        "        self.weights = self.weights - self.learning_rate * dw\n",
        "        self.biases = self.biases - self.learning_rate * dbias\n",
        "        return df\n",
        "    def getname(self):\n",
        "      print(\"Dense \",len(self.weights),len(self.weights[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Wlddt4zmYCVV"
      },
      "outputs": [],
      "source": [
        "class ReLU(Layer):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def forward(self, input):\n",
        "        return np.maximum(0,input)\n",
        "\n",
        "    def backward(self, input, grad_output):\n",
        "        relu_grad = input > 0\n",
        "        return grad_output*relu_grad \n",
        "    def getname(self):\n",
        "      print(\"Relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XNLdPPsvFBIN"
      },
      "outputs": [],
      "source": [
        "class crossEntropy:  \n",
        "  @staticmethod\n",
        "  def softmax(p,y):\n",
        "    return np.exp(p[y])/np.sum(np.exp(p))\n",
        "  def forward(self,p,y):\n",
        "    crossEntropy=[]\n",
        "    for i in range(len(p)):\n",
        "      softmaxvalue=self.softmax(p[i],y[i])\n",
        "      crossEntropy.append(-np.log(softmaxvalue))    \n",
        "    return crossEntropy\n",
        "  def crossEntropy(self,p,y):\n",
        "    crossEntropy=[]\n",
        "    for i in range(len(p)):\n",
        "      softmaxvalue=self.softmax(p[i],y[i])\n",
        "      crossEntropy.append(-np.log(softmaxvalue))    \n",
        "    return crossEntropy\n",
        "  def backward(self,y_pred,y_true):\n",
        "    ones_hot = np.zeros_like(y_pred)\n",
        "    ones_hot[np.arange(len(y_pred)),y_true] = 1\n",
        "    softmax = np.exp(y_pred) / np.exp(y_pred).sum(axis=-1,keepdims=True)\n",
        "    return (- ones_hot + softmax) / y_pred.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(flatten=False):\n",
        "    feature = ['Position Label']\n",
        "    train_feature =data_train.drop(feature, axis=1)\n",
        "    # Set The Target\n",
        "    train_target = data_train[\"Position Label\"]\n",
        "    # Split Data\n",
        "    from sklearn.model_selection import train_test_split, cross_val_score\n",
        "    X_train, X_test, y_train, y_test = train_test_split(train_feature ,train_target, test_size=0.3, random_state=42)\n",
        "    X_train=X_train.to_numpy()\n",
        "    X_test=X_test.to_numpy()\n",
        "    y_train=y_train.to_numpy()\n",
        "    y_test = y_test .to_numpy()\n",
        "    return X_train, X_test, y_train, y_test\n",
        "X_train, X_test,y_train, y_test = load_dataset()"
      ],
      "metadata": {
        "id": "BhWFrxV1dwxN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "  def __init__(self,input,hiddens,activation=\"relu\"):\n",
        "    self.input=input\n",
        "    self.hiddens=hiddens\n",
        "    self.network=[] \n",
        "    self.network.append(Dense(input,hiddens[0]))\n",
        "    self.network.append(ReLU())\n",
        "    for i in range(len(hiddens)-2):\n",
        "      self.network.append(Dense(hiddens[i],hiddens[i+1]))\n",
        "      self.network.append(ReLU())\n",
        "    self.network.append(Dense(hiddens[len(hiddens)-2],hiddens[len(hiddens)-1]))\n",
        "    # self.network = []\n",
        "    # self.network.append(Dense(X_train.shape[1],1024))\n",
        "    # self.network.append(ReLU())\n",
        "    # self.network.append(Dense(1024,512))\n",
        "    # self.network.append(ReLU())\n",
        "    # self.network.append(Dense(512,80))\n",
        "    # print(len(self.network))\n",
        "    # for i in self.network:\n",
        "    #   print(i.name())\n",
        "\n",
        "  def forward(self,X):\n",
        "      activations = []\n",
        "      input = X\n",
        "      for i in range(len(self.network)):\n",
        "          activations.append(self.network[i].forward(X))\n",
        "          X = self.network[i].forward(X)\n",
        "          \n",
        "      assert len(activations) == len(self.network)\n",
        "      return activations\n",
        "\n",
        "  def predict(self,X):\n",
        "      y_pred = self.forward(X)[-1]\n",
        "      return y_pred.argmax(axis=-1)\n",
        "\n",
        "  def train(self,X,y):\n",
        "      layer_activations = self.forward(X)\n",
        "      y_pred = layer_activations[-1]\n",
        "      derivation=crossEntropy()\n",
        "      loss =derivation.forward(y_pred,y)\n",
        "      loss_grad = derivation.backward(y_pred,y)\n",
        "      \n",
        "      for i in range(1, len(self.network)):\n",
        "          loss_grad = self.network[len(self.network) - i].backward(layer_activations[len(self.network) - i - 1], loss_grad)\n",
        "      \n",
        "      return np.mean(loss)"
      ],
      "metadata": {
        "id": "3Hib6mDDd5pR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import trange\n",
        "import logging\n",
        "logging.info(\"hello world\")\n",
        "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
        "    assert len(inputs) == len(targets)\n",
        "    if shuffle:\n",
        "        indices = np.random.permutation(len(inputs)) \n",
        "    for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
        "        if shuffle:\n",
        "            excerpt = indices[start_idx:start_idx + batchsize]\n",
        "        else:\n",
        "            excerpt = slice(start_idx, start_idx + batchsize)\n",
        "        yield inputs[excerpt], targets[excerpt]"
      ],
      "metadata": {
        "id": "q_ePoCapeBvL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_log = []\n",
        "val_log = []\n",
        "loss_log=[]\n",
        "# network = []\n",
        "# network.append(Dense(X_train.shape[1],1024))\n",
        "# network.append(ReLU())\n",
        "# network.append(Dense(1024,512))\n",
        "# network.append(ReLU())\n",
        "# network.append(Dense(512,80))\n",
        "model=MLP(input=X_train.shape[1],hiddens=[1024,512,80])\n",
        "for epoch in range(100):\n",
        "    loss_train=[]  \n",
        "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=64,shuffle= False):\n",
        "        loss=model.train(x_batch,y_batch)\n",
        "        loss_train.append(loss)\n",
        "    loss_log.append(min(loss_train))\n",
        "    acurateTraining=np.mean(model.predict(X_train)==y_train)\n",
        "    train_log.append(acurateTraining)\n",
        "    acurateTesting=np.mean(model.predict(X_test)==y_test)\n",
        "    val_log.append(acurateTesting)\n",
        "    clear_output()\n",
        "    print('epoch ' + str(epoch) + ' ,train_loss ' + str((round(loss,4))) + ' ,acc_training ' + str(round(acurateTraining,4)) + ' ,acc_testing ' + str(round(acurateTesting,4)))\n",
        "print(\"Acurracy training and testing\")\n",
        "plt.plot(train_log,label='train accuracy')\n",
        "plt.plot(val_log,label='val accuracy')\n",
        "plt.legend(loc='best')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "print(\"LOSS value\")\n",
        "plt.plot(loss_log,label=\"Loss\")\n",
        "plt.show()\n",
        "print(\"Accuracy train:\",max(train_log))\n",
        "print(\"Accuracy test:\",max(val_log))\n",
        "print(\"Loss:\",min(loss_log))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85dNJCQgeHNF",
        "outputId": "3dabfb6b-b510-445c-9297-63f602de3e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 25 ,train_loss 1.1454 ,acc_training 0.6073 ,acc_testing 0.6075\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 356/531 [00:10<00:05, 32.16it/s]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyMUokkbJDA9n1xSleUJhUoa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}